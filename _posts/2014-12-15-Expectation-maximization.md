---
layout: post
title: "Expectation maximization"
description: ""
category: static
tags: [static]
---
{% include JB/setup %}
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="https://google-code-prettify.googlecode.com/svn/loader/run_prettify.js"></script>
#基本概念
最大期望算法（Expectation-maximization algorithm，又译期望最大化算法）在统计中被用于寻找，依赖于不可观察的隐性变量的概率模型中，参数的最大似然估计。

在统计计算中，最大期望（EM）算法是在概率（probabilistic）模型中寻找参数最大似然估计或者最大后验估计的算法，其中概率模型依赖于无法观测的隐藏变量（Latent Variable）。最大期望经常用在机器学习和计算机视觉的数据聚类（Data Clustering）领域。最大期望算法经过两个步骤交替进行计算，第一步是计算期望（E），利用对隐藏变量的现有估计值，计算其最大似然估计值；第二步是最大化（M），最大化在 E 步上求得的最大似然值来计算参数的值。M 步上找到的参数估计值被用于下一个 E 步计算中，这个过程不断交替进行。

#Jensen不等式
回顾优化理论中的一些概念。设$$f$$是定义域为实数的函数，如果对于所有的实数$$x$$,$$f^{''}(x)\geq0$$,那么$$f$$是凸函数。当$$x$$是向量时，如果其hessian矩阵H是半正定的$$(H\geq0)$$，那么$$f$$是凸函数。如果$$f^{''}(x)>0$$或者$$H>0$$，那么称$$f$$是严格凸函数。

Jensen不等式表述如下：

如果f是凸函数，X是随机变量，那么$$E[f(X)]\geq f(EX)$$, 特别地，如果$$f$$是严格凸函数，那么$$E[f(X)]=f(EX)$$当且仅当$$p(x=E[X])=1$$，也就是说$$X$$是常量。这里我们将$$f(E[X])$$简写为$$f(EX)$$。

如果用图表示会很清晰：

<div><p><img align='center' src="/img/EM1.png"/></p></div>

 图中，实线$$f$$是凸函数，$$X$$是随机变量，有0.5的概率是a，有0.5的概率是b。（就像掷硬币一样）。$$X$$的期望值就是a和b的中值了，图中可以看到$$E(f(X))\geq f(EX)$$成立。当$$f$$是（严格）凹函数当且仅当$$-f$$是（严格）凸函数。 Jensen不等式应用于凹函数时，不等号方向反向，也就是$$E(f(X))\leq f(EX)$$。
 
#EM算法
 
 给定的训练样本是$${x^{(1)},\dots,x^{x(m)}}$$，样例间独立，我们想找到每个样例隐含的类别$$z$$，能使得$$p(x,z)$$最大。$$p(x,z)$$的最大似然估计如下：
 
 $$l(\theta)=\sum_{i=1}^{m}logp(x;\theta)=\sum_{i=1}^{m}log\sum_zp(x,z,\theta)$$
 
 <strong>第一步是对极大似然取对数，第二步是对每个样例的每个可能类别$$z$$求联合分布概率和</strong>。但是直接求\theta一般比较困难，因为有隐藏变量$$z$$存在，但是一般确定了$$z$$后，求解就容易了。

 EM是一种解决存在隐含变量优化问题的有效方法。竟然不能直接最大化$$l(\theta)$$，我们可以不断地建立$$l$$的下界（E步），然后优化下界（M步）。这句话比较抽象，看下面的。
 
 对于每一个样例i，让$$Q_i$$表示该样例隐含变量$$z$$的某种分布，$$Q_i$$满足的条件是$$\sum_zQ_i(z)=1,Q_i(z)\geq0$$。（如果$$z$$是连续性的，那么$$Q_i$$是概率密度函数，需要将求和符号换做积分符号）。比如要将班上学生聚类，假设隐藏变量$$z$$是身高，那么就是连续的高斯分布。如果按照隐藏变量是男女，那么就是伯努利分布了。
 
 可以由前面阐述的内容得到下面的公式：
 
 $$\sum_ilogp(x^{(i)};l) = \sum_ilog\sum_{z^{(i)}}p(x^{(i)},z^{(i)};\theta)=\sum_ilog\sum_{z^{(i)}}Q_i(z^{(i)})\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}\geq\sum_i\sum_{z^{(i)}}Q_i(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}$$
 
 （1）到（2）比较直接，就是分子分母同乘以一个相等的函数。（2）到（3）利用了Jensen不等式，考虑到$$log(x)$$是凹函数（二阶导数小于0），而且$$\sum_{z^{(i)}}Q_i(z^{(i)})[\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}]$$， 就是$$p(x^{(i)},z^{(i)};\theta)/Q_i(z^{(i)})$$的期望（回想期望公式中的Lazy Statistician规则）。
 
设Y是随机变量X的函数$$Y=g(x)$$（g是连续函数），那么

（1） X是离散型随机变量，它的分布律为$$P(X=x_k)=p_k，k=1,2\dots$$。若$$\sum_{k=1}^{\infty}g(x_k)p_k$$绝对收敛，则有
$$E(Y) = E[g(X)] = \sum_{k=1}^{\infty}g(x_k)p_k$$
	
（2） X是连续型随机变量，它的概率密度为$$f(x)$$，若$$\int_{-\infty}^{\infty}g(x)f(x)dx$$绝对收敛，则有$$E(Y)=E[g(x)]=\int_{-\infty}^{\infty}g(x)f(x)dx$$
	
 对应于上述问题，Y是$$[p(x^{(i)},z^{(i)};\theta)/Q_i(z^{(i)})]$$，X是$$z^{(i)}$$，$$Q_i(z^{(i)})$$是$$p_k$$，g是$$z^{(i)}$$到$$[p(x^{(i)},z^{(i)};\theta)/Q_i(z^{(i)})]$$的映射。这样解释了式子（2）中的期望，再根据凹函数时的Jensen不等式：
  
 $$f(E_{z^{(i)}~Q_i}[\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}]\geq E_{z^{(i)}~Q_i}[f(\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})})]$$,
  
 可以得到（3）。
 
 这个过程可以看作是对$$l(\theta)$$求了下界。对于$$Q_i$$的选择，有多种可能，那种更好的？假设$$\theta$$已经给定，那么$$l(\theta)$$的值就决定于$$Q_i(z^{(i)})$$和$$p(x^{(i)},z^{(i)})$$了。我们可以通过调整这两个概率使下界不断上升，以逼近$$l(\theta)$$的真实值，那么什么时候算是调整好了呢？当不等式变成等式时，说明我们调整后的概率能够等价于$$l(\theta)$$了。按照这个思路，我们要找到等式成立的条件。根据Jensen不等式，要想让等式成立，需要让随机变量变成常数值，这里得到：
 
 $$\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}=c$$
 
 c为常数，不依赖于$$z^{(i)}$$。对此式子做进一步推导，我们知道$$\sum_zQ(z^{(i)})=1$$，那么也就有$$\sum_zp(x^{(i)},z^{(i)};\theta)=c$$，（多个等式分子分母相加不变，这个认为每个样例的两个概率比值都是c），那么有下式：
 
 $$Q_i(z^{(i)})=\frac{p(x^{(i)},z^{(i)};\theta)}{\sum_zp(x^{(i)},z^{(i)};\theta)}=\frac{p(x^{(i)},z^{(i)};\theta)}{p(x^{(i)};\theta)}=p(z^{(i)}|x^{(i)};\theta)$$
 
 至此，我们推出了在固定其他参数$$\theta$$后，$$Q_i(z^{(i)})$$的计算公式就是后验概率，解决了$$Q_i(z^{(i)})$$如何选择的问题。这一步就是E步，建立$$l(\theta)$$的下界。接下来的M步，就是在给定$$Q_i(z^{(i)})$$后，调整$$\theta$$，去极大化$$l(\theta)$$的下界（在固定$$Q_i(z^{(i)})$$后，下界还可以调整的更大）。那么一般的EM算法的步骤如下：
  
 循环重复直到收敛 {
  
（E步）对于每一个i，计算$$Q_i(z^{(i)}):=p(x^{(i)},z^{(i)};\theta)$$
  
（M步）计算 $$\theta:=argmax_{\theta}\sum_i\sum_{z^{(i)}}Q_i(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}$$
  
这里顺便提一下其中的p的计算式可以实例化 例如p的公式可以被 贝叶斯公式替代 另外对于z和$$\theta$$的初始值，有的资料给出的办法是第一次猜测隐含类别变量z，对于$$\theta$$可以复给一个随意的初始值
那么究竟怎么确保EM收敛？假定$$\theta^{(t)}$$和$$\theta^{(t+1))$$是EM第t次和t+1次迭代后的结果。如果我们证明了$$l(\theta^{(t)})\leql(\theta^{(t+1)})$$，也就是说极大似然估计单调增加，那么最终我们会到达最大似然估计的最大值。下面来证明，选定$$\theta^{(t)}$$后，我们得到E步

$$Q_i^{(t)}(z^{(i)}):=p(z^{(i)}|x^{(i)};\theta^{(t)})$$

这一步保证了在给定$$\theta^{(t)}$$时，Jensen不等式中的等式成立，也就是

$$l(\theta^{(t)})=\sum_i\sum_{z^{(i)}}Q_i^{(t)}(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\theta^{(t)})}{Q_i^{(t)}(z^{(i)})}$$

然后进行M步，固定$$Q_i^{(t)}(z^{(i)})$$，并将$$\theta^(t)$$视作变量，对上面的$$l(\theta^{(t)})$$求导后，得到$$\theta^{(t+1)}$$，这样经过一些推导会有以下式子成立：

$$l(\theta^{(t+1)}) \geq \sum_i\sum_{z^{(i)}}Q_i^{(t)}(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\theta^{(t+1)})}{Q_i^{(t)}(z^{(i)})}\geq\sum_i\sum_{z^{(i)}}Q_i^{(t)}(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\theta^{(t)})}{Q_i^{(t)}(z^{(i)})}=l(\theta^{(t)})$$

解释第（1）步，得到$$\theta^{(t+1)}$$，只是最大化$$l(\theta^{(t)})$$，也就是$$l(\theta^{(t+1)})$$的下界，而没有使等式成立，等式成立只有是在固定$$\theta$$，并按E步得到$$Q_i$$时才能成立。

况且根据我们前面得到的下式，对于所有的$$Q_i$$和$$\theta$$都成立

$$l(\theta)\geq\sum_i\sum_{z^{(i)}}Q_i(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}$$

 第（2）步利用了M步的定义，M步就是将$$\theta^{(t)}$$调整到$$\theta^{(t+1)}$$，使得下界最大化。因此（2）成立，（3）是之前的等式结果。
 
这样就证明了$$l(\theta)$$会单调增加。一种收敛方法是$$l(\theta)$$不再变化，还有一种就是变化幅度很小。
  
再次解释一下（1）、（2）、（3）。首先（1）对所有的参数都满足，而其等式成立条件只是在固定$$\theta$$，并调整好Q时成立，而第（1）步只是固定Q，调整$$\theta$$，不能保证等式一定成立。（1）到（2）就是M步的定义，（2）到（3）是前面E步所保证等式成立条件。也就是说E步会将下界拉到与$$l(\theta)$$一个特定值（这里$$\theta^{(t)}$$）一样的高度，而此时发现下界仍然可以上升，因此经过M步后，下界又被拉升，但达不到与$$l(\theta)$$另外一个特定值一样的高度，之后E步又将下界拉到与这个特定值一样的高度，重复下去，直到最大值。

如果我们定义

$$J(Q,\theta)=\sum_i\sum_{z^{(i)}}Q_i(z^{(i)})log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}$$

从前面的推导中我们知道$$l(\theta)\geq J(Q,\theta)，EM可以看作是J的坐标上升法，E步固定$$\theta$$，优化$$Q$$，M步固定$$Q$$优化$$\theta$$。



$$Q_i(z^{(i)})=\frac{p(x^{(i)},z^{(i)};\theta)}{\sum_zp(x^{(i)},z^{(i)};\theta)}=\frac{p(x^{(i)},z^{(i)};\theta)}{p(x^{(i)};\theta)}=p(z^{(i)}|x^{(i)};\theta)$$